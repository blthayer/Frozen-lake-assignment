{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "\n",
    "env = gym.make(\"FrozenLake-v0\")\n",
    "\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to play with these hyperparameters\n",
    "\n",
    "total_episodes = 15000        # Total episodes\n",
    "test_episodes = 10            # Test episodes\n",
    "learning_rate = 0.8           # Learning rate\n",
    "max_steps = 99                # Max steps per episode\n",
    "gamma = 0.95                  # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.01            # Minimum exploration probability \n",
    "decay_rate = 0.005            # Exponential decay rate for exploration prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 0.4693333333333333\n",
      "Q values:\n",
      "[[2.66545578e-01 6.67885834e-02 1.16357648e-01 1.10258720e-01]\n",
      " [1.51647028e-02 1.24327446e-04 5.12285322e-03 1.34782140e-01]\n",
      " [1.00998980e-01 1.26920831e-02 1.10422166e-02 1.24667448e-02]\n",
      " [6.64107209e-03 1.41900315e-02 8.85724453e-03 1.31251811e-02]\n",
      " [2.68976824e-01 2.69697028e-02 2.65214854e-02 8.32934520e-03]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.43171003e-03 9.55947511e-06 5.29088630e-02 1.12117083e-09]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [2.08229745e-01 2.59400285e-02 5.48858720e-02 4.82784342e-01]\n",
      " [5.51441502e-02 5.47870192e-01 1.02787017e-03 3.72546921e-02]\n",
      " [8.63886230e-01 1.05814218e-01 2.04048089e-02 1.54393092e-02]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [8.44021267e-02 1.71855295e-01 8.14585662e-01 8.45551130e-02]\n",
      " [1.79794722e-01 9.84217469e-01 2.02966651e-01 6.26043017e-02]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Initializations\n",
    "qtable = np.zeros((state_size, action_size))\n",
    "rewards = []\n",
    "\n",
    "for episode in range(total_episodes):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Choose an action a in the current state (greedy or explore)\n",
    "        \n",
    "        exp_exp_tradeoff = random.uniform(0, 1)  \n",
    "        # exploitation (taking the max Q value for this state)\n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "            # Enter code here\n",
    "            ## Hint: Greedily choose an action according to Q value\n",
    "\n",
    "            ############################################################\n",
    "            # Select the maximum q-value for this action.\n",
    "            action = np.argmax(qtable[state, :])\n",
    "            ############################################################\n",
    "        # exploration\n",
    "        else:\n",
    "            # Enter code here\n",
    "            ## Hint: Randomly choose an action\n",
    "            \n",
    "            ############################################################\n",
    "            # Draw random action.\n",
    "            action = env.action_space.sample()\n",
    "            ############################################################\n",
    "\n",
    "        # Take this action and observe\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Do a Q update\n",
    "        # Enter code here\n",
    "        ## Hint: One line update equation convert to one line code, start with \"qtable[state, action] = ...\"\n",
    "        \n",
    "        ################################################################\n",
    "        # Use equation 21.8 in the textbook.\n",
    "        # Alternative is on slide 81 in the RL lecture slides. \n",
    "        qtable[state, action] = \\\n",
    "            qtable[state, action] \\\n",
    "            + learning_rate \\\n",
    "            * (\n",
    "                    reward + gamma * qtable[new_state, :].max()\n",
    "                    - qtable[state, action]\n",
    "            )\n",
    "        ################################################################\n",
    "        \n",
    "        total_rewards += reward\n",
    "        \n",
    "        state = new_state\n",
    "        \n",
    "        if done == True: \n",
    "            break\n",
    "        \n",
    "    # Decay epsilon to reduce exploration as time progresses\n",
    "    \n",
    "    # Enter code here to assign a decay value to \"decay_parameter\"\n",
    "    \n",
    "    ## Hint: \n",
    "    ## 1. Use inbuilt polynomial, exponential(, or whatever works) functions to decay epsilon\n",
    "    ## 2. \"decay_parameter\" is a function of \"decay_rate\" and \"episode\"\n",
    "    \n",
    "    ####################################################################\n",
    "    decay_parameter = np.exp(-decay_rate * episode)\n",
    "    ####################################################################\n",
    "    \n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*decay_parameter\n",
    "    rewards.append(total_rewards)\n",
    "\n",
    "print(\"Score over time: \" +  str(sum(rewards)/total_episodes))\n",
    "print(\"Q values:\")\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. In short, explain why fixed \"epsilon\" above isn't the best choice? (Hint: You can keep epsilon fixed and see whether your reasoning explains the behavior)\n",
    "\n",
    "\n",
    "Answer:\n",
    "\n",
    "As time goes on, the algorithm moves toward the optimal policy. If we fix epsilon, our explotation/exploration ratio will be fixed in the long run. With a fixed epsilon, the whole space will be explored, but we'll take more suboptimal actions than is necessary. By decaying epsilon, we reduce exploration as time goes on, which allows us to converge to the optimal policy faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We only print the last state in each episode, to see if our agent has reached the destination or fallen into a hole\n",
      "****************************************************\n",
      "EPISODE  0\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Number of steps 50\n",
      "****************************************************\n",
      "EPISODE  1\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Number of steps 40\n",
      "****************************************************\n",
      "EPISODE  2\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Number of steps 19\n",
      "****************************************************\n",
      "EPISODE  3\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Number of steps 11\n",
      "****************************************************\n",
      "EPISODE  4\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Number of steps 19\n",
      "****************************************************\n",
      "EPISODE  5\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Number of steps 47\n",
      "****************************************************\n",
      "EPISODE  6\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Number of steps 16\n",
      "****************************************************\n",
      "EPISODE  7\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Number of steps 76\n",
      "****************************************************\n",
      "EPISODE  8\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Number of steps 61\n",
      "****************************************************\n",
      "EPISODE  9\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Number of steps 28\n"
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "#################### Final policy animation ############################\n",
    "########################################################################\n",
    "\n",
    "print(\"We only print the last state in each episode, to see if our agent has reached the destination or fallen into a hole\")\n",
    "env.reset()\n",
    "\n",
    "for episode in range(test_episodes):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    print(\"****************************************************\")\n",
    "    print(\"EPISODE \", episode)\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        # Taking action with Q learning\n",
    "        action = np.argmax(qtable[state,:])\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            env.render()\n",
    "            \n",
    "            print(\"Number of steps\", step)\n",
    "            break\n",
    "        state = new_state\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. In some episodes above, the policy isn't reaching the goal, why?\n",
    "\n",
    "Answer:\n",
    "\n",
    "Unlike in the \"PI\" code, the lake here is \"slippery.\" In other words, there's a chance our actions don't take us where we want to go. So while the final policy is optimal, there will be times when the agent falls in a hole due to the non-deterministic nature of the frozen lake. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
